{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import string\n",
    "\n",
    "import tweepy                  #Getting Twitter data like tweets, followers, friends\n",
    "\n",
    "import nltk\n",
    "from textblob import TextBlob  #Sentiment Analysis\n",
    "\n",
    "import networkx as nx          #Drawing Network\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_credentials = dict()\n",
    "#These are the credentials obtained by setting up twitter developer account\n",
    "t_credentials['CONSUMER_KEY'] = '-----------------'\n",
    "t_credentials['CONSUMER_SECRET'] = '-----------------'\n",
    "t_credentials['ACCESS_KEY'] = '-----------------'\n",
    "t_credentials['ACCESS_SECRET'] = '-----------------'\n",
    "\n",
    "#load Twitter API credentials\n",
    "consumer_key = t_credentials['CONSUMER_KEY']\n",
    "consumer_secret = t_credentials['CONSUMER_SECRET']\n",
    "access_key = t_credentials['ACCESS_KEY']\n",
    "access_secret = t_credentials['ACCESS_SECRET']\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweet data is collected using this cell. \n",
    "#Since the data is already collected and stored in brexit_tweets_april.csv, no need to run this cell.\n",
    "#Just read the data from brexit_tweets_april.csv file which is done below.\n",
    "'''\n",
    "\n",
    "hash_tags = '#brexit -filter:retweets'\n",
    "tweet_cols = ['created_at','id','screen_name','location','followers_count','friends_count','retweeted','retweet_count',\n",
    "              'text','tags','mentions']\n",
    "tweet_df = pd.DataFrame(columns = tweet_cols)\n",
    "for tweet in tweepy.Cursor(api.search,q=hash_tags, result_type='recent', # Example Values: mixed, recent, popular\n",
    "                           lang=\"en\",tweet_mode='extended',until='2019-04-30',wait_on_rate_limit=True).items(400000):\n",
    "    tags=[]\n",
    "    for i in range(len(tweet.entities['hashtags'])):\n",
    "        tags.append('#'+tweet.entities['hashtags'][i]['text'].lower())\n",
    "    mentions = []\n",
    "    for i in range(len(tweet.entities['user_mentions'])):\n",
    "        mentions.append('@'+tweet.entities['user_mentions'][i]['screen_name'])\n",
    "    df = pd.DataFrame([[tweet.created_at,tweet.id,tweet.user.screen_name,tweet.user.location,tweet.user.followers_count,\n",
    "                        tweet.user.friends_count,tweet.retweeted,tweet.retweet_count,tweet.full_text,tags,mentions]],columns = tweet_cols)\n",
    "    tweet_df = tweet_df.append(df)\n",
    "    tweet_df_rows = tweet_df.shape[0]\n",
    "    if(tweet_df_rows%100==0):\n",
    "        print(str(tweet_df_rows)+'---'+str(df['created_at']))\n",
    "tweet_df.reset_index(drop=True,inplace=True)\n",
    "tweet_df = tweet_df.sort_values(['created_at'],ascending=[False])\n",
    "print(tweet_df.shape)\n",
    "print(tweet_df['created_at'].min())\n",
    "print(tweet_df['created_at'].max())\n",
    "print(tweet_df['retweet_count'].max())\n",
    "tweet_df.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = pd.read_csv('Outputs/brexit_tweets_april.csv',lineterminator='\\n')\n",
    "tweet_df['mentions'] = tweet_df['mentions\\r'].str.strip()\n",
    "tweet_df['dummy_count'] = 1\n",
    "tweet_df = tweet_df.drop(['Unnamed: 0','id','location','retweeted','mentions\\r'],axis='columns')\n",
    "print(tweet_df.shape)\n",
    "print(tweet_df['created_at'].min())\n",
    "print(tweet_df['created_at'].max())\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(tweet_df['screen_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweet_threshold = 10  #300 worked well for first 3 weeks. Produced 42 nodes\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "def fill_sentiments(tweet_df):\n",
    "    tweet_df['tweet_sentiment_polarity'] = 0.0\n",
    "    tweet_df['tweet_sentiment_subjectivity'] = 0.0\n",
    "    for index,row in tweet_df.iterrows():\n",
    "        cleaned_tweet = row['text']\n",
    "        s_analysis = TextBlob(cleaned_tweet)\n",
    "        tweet_df.at[index,'tweet_sentiment_polarity'] = s_analysis.sentiment.polarity\n",
    "        tweet_df.at[index,'tweet_sentiment_subjectivity'] = s_analysis.sentiment.subjectivity\n",
    "    return tweet_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 1 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df_week1 = tweet_df[tweet_df['created_at']<='2019-04-06 23:59:59']\n",
    "tweet_df_week1.reset_index(drop=True,inplace=True)\n",
    "tweet_df_week1 = fill_sentiments(tweet_df_week1)\n",
    "print(tweet_df_week1.shape)\n",
    "print(tweet_df_week1['created_at'].min())\n",
    "print(tweet_df_week1['created_at'].max())\n",
    "tweet_df_week1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets_df_week1 = tweet_df_week1[['screen_name','retweet_count']].groupby('screen_name').sum()\n",
    "followers_df_week1 = tweet_df_week1[['screen_name','followers_count']].groupby('screen_name').max() #Max of the week\n",
    "friends_df_week1 = tweet_df_week1[['screen_name','friends_count']].groupby('screen_name').max()  #Max of the week\n",
    "sentiment_polarity_df_week1 = tweet_df_week1[['screen_name','tweet_sentiment_polarity']].groupby('screen_name').sum()\n",
    "sentiment_subjectivity_df_week1 = tweet_df_week1[['screen_name','tweet_sentiment_subjectivity']].groupby('screen_name').sum()\n",
    "tweet_count_df_week1 = tweet_df_week1[['screen_name','dummy_count']].groupby('screen_name').sum()\n",
    "\n",
    "week1_stats_df = pd.concat([tweet_count_df_week1,retweets_df_week1,followers_df_week1,friends_df_week1,\n",
    "                            sentiment_polarity_df_week1,sentiment_subjectivity_df_week1],axis='columns')\n",
    "week1_stats_df['screen_name'] = week1_stats_df.index\n",
    "week1_stats_df = week1_stats_df[['screen_name','dummy_count','retweet_count','followers_count','friends_count',\n",
    "                                 'tweet_sentiment_polarity','tweet_sentiment_subjectivity']]\n",
    "week1_stats_df = week1_stats_df.rename(columns={\"retweet_count\":\"total_retweet_count\",\"followers_count\":\"max_followers_count\",\n",
    "                                                \"friends_count\":\"max_friends_count\",\"dummy_count\":\"total_tweet_count\",\n",
    "                                                \"tweet_sentiment_polarity\":\"agg_sentiment_polarity\",\n",
    "                                                \"tweet_sentiment_subjectivity\":\"agg_sentiment_subjectivity\"})\n",
    "week1_stats_df = week1_stats_df[week1_stats_df['total_retweet_count']>=retweet_threshold]\n",
    "week1_stats_df['agg_sentiment_polarity'] = week1_stats_df['agg_sentiment_polarity']/week1_stats_df['total_tweet_count']\n",
    "week1_stats_df['agg_sentiment_subjectivity'] = week1_stats_df['agg_sentiment_subjectivity']/week1_stats_df['total_tweet_count']\n",
    "week1_stats_df.reset_index(drop=True,inplace=True)\n",
    "print(week1_stats_df['agg_sentiment_polarity'].max())\n",
    "print(week1_stats_df['agg_sentiment_polarity'].min())\n",
    "print(week1_stats_df.shape)\n",
    "week1_stats_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 2 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df_week2 = tweet_df[(tweet_df['created_at']>'2019-04-06 23:59:59') & (tweet_df['created_at']<='2019-04-13 23:59:59')]\n",
    "tweet_df_week2.reset_index(drop=True,inplace=True)\n",
    "tweet_df_week2 = fill_sentiments(tweet_df_week2)\n",
    "print(tweet_df_week2.shape)\n",
    "print(tweet_df_week2['created_at'].min())\n",
    "print(tweet_df_week2['created_at'].max())\n",
    "tweet_df_week2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets_df_week2 = tweet_df_week2[['screen_name','retweet_count']].groupby('screen_name').sum()\n",
    "followers_df_week2 = tweet_df_week2[['screen_name','followers_count']].groupby('screen_name').max() #Max of the week\n",
    "friends_df_week2 = tweet_df_week2[['screen_name','friends_count']].groupby('screen_name').max()  #Max of the week\n",
    "sentiment_polarity_df_week2 = tweet_df_week2[['screen_name','tweet_sentiment_polarity']].groupby('screen_name').sum()\n",
    "sentiment_subjectivity_df_week2 = tweet_df_week2[['screen_name','tweet_sentiment_subjectivity']].groupby('screen_name').sum()\n",
    "tweet_count_df_week2 = tweet_df_week2[['screen_name','dummy_count']].groupby('screen_name').sum()\n",
    "\n",
    "week2_stats_df = pd.concat([tweet_count_df_week2,retweets_df_week2,followers_df_week2,friends_df_week2,\n",
    "                            sentiment_polarity_df_week2,sentiment_subjectivity_df_week2],axis='columns')\n",
    "week2_stats_df['screen_name'] = week2_stats_df.index\n",
    "week2_stats_df = week2_stats_df[['screen_name','dummy_count','retweet_count','followers_count','friends_count',\n",
    "                                 'tweet_sentiment_polarity','tweet_sentiment_subjectivity']]\n",
    "week2_stats_df = week2_stats_df.rename(columns={\"retweet_count\":\"total_retweet_count\",\"followers_count\":\"max_followers_count\",\n",
    "                                                \"friends_count\":\"max_friends_count\",\"dummy_count\":\"total_tweet_count\",\n",
    "                                                \"tweet_sentiment_polarity\":\"agg_sentiment_polarity\",\n",
    "                                                \"tweet_sentiment_subjectivity\":\"agg_sentiment_subjectivity\"})\n",
    "week2_stats_df = week2_stats_df[week2_stats_df['total_retweet_count']>=retweet_threshold]\n",
    "week2_stats_df['agg_sentiment_polarity'] = week2_stats_df['agg_sentiment_polarity']/week2_stats_df['total_tweet_count']\n",
    "week2_stats_df['agg_sentiment_subjectivity'] = week2_stats_df['agg_sentiment_subjectivity']/week2_stats_df['total_tweet_count']\n",
    "week2_stats_df.reset_index(drop=True,inplace=True)\n",
    "print(week2_stats_df['agg_sentiment_polarity'].max())\n",
    "print(week2_stats_df['agg_sentiment_polarity'].min())\n",
    "print(week2_stats_df.shape)\n",
    "week2_stats_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 3 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df_week3 = tweet_df[(tweet_df['created_at']>'2019-04-13 23:59:59') & (tweet_df['created_at']<='2019-04-20 23:59:59')]\n",
    "tweet_df_week3.reset_index(drop=True,inplace=True)\n",
    "tweet_df_week3 = fill_sentiments(tweet_df_week3)\n",
    "print(tweet_df_week3.shape)\n",
    "print(tweet_df_week3['created_at'].min())\n",
    "print(tweet_df_week3['created_at'].max())\n",
    "tweet_df_week3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets_df_week3 = tweet_df_week3[['screen_name','retweet_count']].groupby('screen_name').sum()\n",
    "followers_df_week3 = tweet_df_week3[['screen_name','followers_count']].groupby('screen_name').max() #Max of the week\n",
    "friends_df_week3 = tweet_df_week3[['screen_name','friends_count']].groupby('screen_name').max()  #Max of the week\n",
    "sentiment_polarity_df_week3 = tweet_df_week3[['screen_name','tweet_sentiment_polarity']].groupby('screen_name').sum()\n",
    "sentiment_subjectivity_df_week3 = tweet_df_week3[['screen_name','tweet_sentiment_subjectivity']].groupby('screen_name').sum()\n",
    "tweet_count_df_week3 = tweet_df_week3[['screen_name','dummy_count']].groupby('screen_name').sum()\n",
    "\n",
    "week3_stats_df = pd.concat([tweet_count_df_week3,retweets_df_week3,followers_df_week3,friends_df_week3,\n",
    "                            sentiment_polarity_df_week3,sentiment_subjectivity_df_week3],axis='columns')\n",
    "week3_stats_df['screen_name'] = week3_stats_df.index\n",
    "week3_stats_df = week3_stats_df[['screen_name','dummy_count','retweet_count','followers_count','friends_count',\n",
    "                                 'tweet_sentiment_polarity','tweet_sentiment_subjectivity']]\n",
    "week3_stats_df = week3_stats_df.rename(columns={\"retweet_count\":\"total_retweet_count\",\"followers_count\":\"max_followers_count\",\n",
    "                                                \"friends_count\":\"max_friends_count\",\"dummy_count\":\"total_tweet_count\",\n",
    "                                                \"tweet_sentiment_polarity\":\"agg_sentiment_polarity\",\n",
    "                                                \"tweet_sentiment_subjectivity\":\"agg_sentiment_subjectivity\"})\n",
    "week3_stats_df = week3_stats_df[week3_stats_df['total_retweet_count']>=retweet_threshold]\n",
    "week3_stats_df['agg_sentiment_polarity'] = week3_stats_df['agg_sentiment_polarity']/week3_stats_df['total_tweet_count']\n",
    "week3_stats_df['agg_sentiment_subjectivity'] = week3_stats_df['agg_sentiment_subjectivity']/week3_stats_df['total_tweet_count']\n",
    "week3_stats_df.reset_index(drop=True,inplace=True)\n",
    "print(week3_stats_df['agg_sentiment_polarity'].max())\n",
    "print(week3_stats_df['agg_sentiment_polarity'].min())\n",
    "print(week3_stats_df.shape)\n",
    "week3_stats_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 4 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df_week4 = tweet_df[tweet_df['created_at']>'2019-04-23 23:59:59']\n",
    "tweet_df_week4.reset_index(drop=True,inplace=True)\n",
    "tweet_df_week4 = fill_sentiments(tweet_df_week4)\n",
    "print(tweet_df_week4.shape)\n",
    "print(tweet_df_week4['created_at'].min())\n",
    "print(tweet_df_week4['created_at'].max())\n",
    "tweet_df_week4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets_df_week4 = tweet_df_week4[['screen_name','retweet_count']].groupby('screen_name').sum()\n",
    "followers_df_week4 = tweet_df_week4[['screen_name','followers_count']].groupby('screen_name').max() #Max of the week\n",
    "friends_df_week4 = tweet_df_week4[['screen_name','friends_count']].groupby('screen_name').max()  #Max of the week\n",
    "sentiment_polarity_df_week4 = tweet_df_week4[['screen_name','tweet_sentiment_polarity']].groupby('screen_name').sum()\n",
    "sentiment_subjectivity_df_week4 = tweet_df_week4[['screen_name','tweet_sentiment_subjectivity']].groupby('screen_name').sum()\n",
    "tweet_count_df_week4 = tweet_df_week4[['screen_name','dummy_count']].groupby('screen_name').sum()\n",
    "\n",
    "week4_stats_df = pd.concat([tweet_count_df_week4,retweets_df_week4,followers_df_week4,friends_df_week4,\n",
    "                            sentiment_polarity_df_week4,sentiment_subjectivity_df_week4],axis='columns')\n",
    "week4_stats_df['screen_name'] = week4_stats_df.index\n",
    "week4_stats_df = week4_stats_df[['screen_name','dummy_count','retweet_count','followers_count','friends_count',\n",
    "                                 'tweet_sentiment_polarity','tweet_sentiment_subjectivity']]\n",
    "week4_stats_df = week4_stats_df.rename(columns={\"retweet_count\":\"total_retweet_count\",\"followers_count\":\"max_followers_count\",\n",
    "                                                \"friends_count\":\"max_friends_count\",\"dummy_count\":\"total_tweet_count\",\n",
    "                                                \"tweet_sentiment_polarity\":\"agg_sentiment_polarity\",\n",
    "                                                \"tweet_sentiment_subjectivity\":\"agg_sentiment_subjectivity\"})\n",
    "week4_stats_df = week4_stats_df[week4_stats_df['total_retweet_count']>=retweet_threshold]\n",
    "week4_stats_df['agg_sentiment_polarity'] = week4_stats_df['agg_sentiment_polarity']/week4_stats_df['total_tweet_count']\n",
    "week4_stats_df['agg_sentiment_subjectivity'] = week4_stats_df['agg_sentiment_subjectivity']/week4_stats_df['total_tweet_count']\n",
    "week4_stats_df.reset_index(drop=True,inplace=True)\n",
    "print(week4_stats_df['agg_sentiment_polarity'].max())\n",
    "print(week4_stats_df['agg_sentiment_polarity'].min())\n",
    "print(week4_stats_df.shape)\n",
    "week4_stats_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check how many people have tweeted #Brexit for each week of April"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here don't take into account of the retweet_threshold\n",
    "week1_tweeters = set(tweet_df_week1['screen_name'])\n",
    "print(\"Number of #brexit (en) tweeters in week 1 is: \" + str(len(week1_tweeters)))\n",
    "week2_tweeters = set(tweet_df_week2['screen_name'])\n",
    "print(\"Number of #brexit (en) tweeters in week 2 is: \" + str(len(week2_tweeters)))\n",
    "week3_tweeters = set(tweet_df_week3['screen_name'])\n",
    "print(\"Number of #brexit (en) tweeters in week 3 is: \" + str(len(week3_tweeters)))\n",
    "week4_tweeters = set(tweet_df_week4['screen_name'])\n",
    "print(\"Number of #brexit (en) tweeters in week 4 is: \" + str(len(week4_tweeters)))\n",
    "\n",
    "all_week_tweeters = week1_tweeters & week2_tweeters & week3_tweeters & week4_tweeters\n",
    "print(\"Number of common #brexit (en) tweeters for all weeks is: \" + str(len(all_week_tweeters)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Screen only the most popular twitter users whose aggregate retweets for the week > retweet_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week1_popular_tweeters = set(week1_stats_df['screen_name'])\n",
    "print(\"Number of #brexit (en + retweet_threshold) tweeters in week 1 is: \" + str(len(week1_popular_tweeters)))\n",
    "week2_popular_tweeters = set(week2_stats_df['screen_name'])\n",
    "print(\"Number of #brexit (en + retweet_threshold) tweeters in week 2 is: \" + str(len(week2_popular_tweeters)))\n",
    "week3_popular_tweeters = set(week3_stats_df['screen_name'])\n",
    "print(\"Number of #brexit (en + retweet_threshold) tweeters in week 3 is: \" + str(len(week3_popular_tweeters)))\n",
    "week4_popular_tweeters = set(week4_stats_df['screen_name'])\n",
    "print(\"Number of #brexit (en + retweet_threshold) tweeters in week 4 is: \" + str(len(week4_popular_tweeters)))\n",
    "\n",
    "all_week_popular_tweeters = week1_popular_tweeters & week2_popular_tweeters & week3_popular_tweeters & week4_popular_tweeters\n",
    "print(\"Number of common #brexit (en + retweet_threshold) tweeters for all weeks is: \" + str(len(all_week_popular_tweeters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(all_week_popular_tweeters)\n",
    "def create_pairings(source):\n",
    "        result = []\n",
    "        for p1 in range(len(source)):\n",
    "                for p2 in range(p1+1,len(source)):\n",
    "                        result.append([source[p1],source[p2]])\n",
    "        return result\n",
    "\n",
    "pairings = create_pairings(nodes)\n",
    "print(\"%d pairings\" % len(pairings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_name_cols = ['source_screen_name','destination_screen_name']\n",
    "network_df = pd.DataFrame(pairings, columns = screen_name_cols)\n",
    "network_df['has_mutual_following'] = False #Initialize it to false then compute the follower friend mutual relations\n",
    "network_df['source_follow_dest'] = False\n",
    "network_df['dest_follow_source'] = False  #source is a friend of dest\n",
    "print(network_df.shape)\n",
    "network_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the mutual following info among the list of popular Twitter users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This takes time. Takes about 1 hour for finding connection between every 750 pair of nodes\n",
    "'''\n",
    "for index,row in network_df.iterrows():\n",
    "    ff_rel = api.show_friendship(source_screen_name=row['source_screen_name'], target_screen_name=row['destination_screen_name'])\n",
    "    network_df.at[index,'has_mutual_following'] = (ff_rel[0].followed_by == True and ff_rel[0].following == True)\n",
    "    network_df.at[index,'source_follow_dest'] = (ff_rel[0].following == True)\n",
    "    network_df.at[index,'dest_follow_source'] = (ff_rel[0].followed_by == True)\n",
    "\n",
    "print(network_df.shape)\n",
    "network_df.head()\n",
    "network_df.to_csv('Outputs\\mutual_folling_info_retweet_thresh_'+str(retweet_threshold)+'.csv')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df = pd.read_csv('Outputs\\mutual_folling_info_retweet_thresh_'+str(retweet_threshold)+'.csv') #load from saved\n",
    "network_df = network_df.drop(columns='Unnamed: 0',axis=1)\n",
    "print(network_df.shape)\n",
    "network_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df['has_mutual_following'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df['source_follow_dest'].sum() + network_df['dest_follow_source'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_stats_df_week1 = week1_stats_df[week1_stats_df['screen_name'].isin(all_week_popular_tweeters)]\n",
    "network_stats_df_week1.reset_index(drop=True,inplace=True)\n",
    "\n",
    "network_stats_df_week2 = week2_stats_df[week2_stats_df['screen_name'].isin(all_week_popular_tweeters)]\n",
    "network_stats_df_week2.reset_index(drop=True,inplace=True)\n",
    "\n",
    "network_stats_df_week3 = week3_stats_df[week3_stats_df['screen_name'].isin(all_week_popular_tweeters)]\n",
    "network_stats_df_week3.reset_index(drop=True,inplace=True)\n",
    "\n",
    "network_stats_df_week4 = week4_stats_df[week4_stats_df['screen_name'].isin(all_week_popular_tweeters)]\n",
    "network_stats_df_week4.reset_index(drop=True,inplace=True)\n",
    "\n",
    "network_stats_df_week1.to_csv('Outputs\\week_1_network_retweet_thresh_'+str(retweet_threshold)+'.csv')\n",
    "network_stats_df_week2.to_csv('Outputs\\week_2_network_retweet_thresh_'+str(retweet_threshold)+'.csv')\n",
    "network_stats_df_week3.to_csv('Outputs\\week_3_network_retweet_thresh_'+str(retweet_threshold)+'.csv')\n",
    "network_stats_df_week4.to_csv('Outputs\\week_4_network_retweet_thresh_'+str(retweet_threshold)+'.csv')\n",
    "\n",
    "print(network_stats_df_week1.shape)\n",
    "network_stats_df_week1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network for week 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the mutual relation using networkX library\n",
    "nodes = list(network_stats_df_week1['screen_name'])\n",
    "pd.DataFrame(nodes,columns=['Twitter_users']).to_csv('nodes_list_retweet_thresh_'+str(retweet_threshold)+'.csv')\n",
    "size_of_nodes = list(network_stats_df_week1['total_retweet_count'])\n",
    "color_of_nodes = list(network_stats_df_week1['agg_sentiment_polarity']*2) #Multiply by 2 to see more contrast in colors\n",
    "mutual_follow_lol = network_df[['source_screen_name','destination_screen_name']][network_df['has_mutual_following']==True].values.tolist()\n",
    "pd.DataFrame(mutual_follow_lol,columns=['Twitter_user_1','Twitter_user_2']).to_csv('Outputs\\edges_list_retweet_thresh_'+str(retweet_threshold)+'.csv')\n",
    "mutual_follow_edges = []\n",
    "for mfe in mutual_follow_lol:\n",
    "    mutual_follow_edges.append((mfe[0],mfe[1]))\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(mutual_follow_edges)\n",
    "plt.figure(figsize=(75,75))\n",
    "pos = nx.spring_layout(G, k=1.5*1/np.sqrt(len(G.nodes())), iterations=30)\n",
    "nx.draw(G,pos=pos,with_labels = True,font_size=50,node_size=size_of_nodes,node_color=color_of_nodes,\n",
    "        cmap=plt.get_cmap('YlOrBr'),seed=10)\n",
    "plt.title('''Popular Social Network Graph of people who have tweeted #Brexit on 1st week of April\n",
    "             Size of nodes indicates number of retweets\n",
    "             Edges indicates mutual following relation\n",
    "             Color of nodes indicates sentiment''',fontsize=100)\n",
    "plt.savefig(\"network_graph_week1_retweet_thresh_\"+str(retweet_threshold)+\".jpeg\") #save as jpeg\n",
    "plt.show() #display\n",
    "#Intensive color means the sentiment for #Brexit is positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network for week 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the mutual relation using networkX library\n",
    "nodes = list(network_stats_df_week2['screen_name'])\n",
    "size_of_nodes = list(network_stats_df_week2['total_retweet_count'])\n",
    "color_of_nodes = list(network_stats_df_week2['agg_sentiment_polarity']*2) #Multiply by 2 to see more contrast in colors\n",
    "mutual_follow_lol = network_df[['source_screen_name','destination_screen_name']][network_df['has_mutual_following']==True].values.tolist()\n",
    "mutual_follow_edges = []\n",
    "for mfe in mutual_follow_lol:\n",
    "    mutual_follow_edges.append((mfe[0],mfe[1]))\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(mutual_follow_edges)\n",
    "plt.figure(figsize=(75,75))\n",
    "pos = nx.spring_layout(G, k=1.5*1/np.sqrt(len(G.nodes())), iterations=20)\n",
    "nx.draw(G,pos=pos,with_labels = True,font_size=50,node_size=size_of_nodes,node_color=color_of_nodes,\n",
    "        cmap=plt.get_cmap('YlOrBr'),seed=10)\n",
    "plt.title('''Popular Social Network Graph of people who have tweeted #Brexit on 2nd week of April\n",
    "             Size of nodes indicates number of retweets\n",
    "             Edges indicates mutual following relation\n",
    "             Color of nodes indicates sentiment''',fontsize=100)\n",
    "plt.savefig(\"network_graph_week2_retweet_thresh_\"+str(retweet_threshold)+\".jpeg\") #save as jpeg\n",
    "plt.show() #display\n",
    "#Intensive color means the sentiment for #Brexit is positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network for week 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the mutual relation using networkX library\n",
    "nodes = list(network_stats_df_week3['screen_name'])\n",
    "size_of_nodes = list(network_stats_df_week3['total_retweet_count'])\n",
    "color_of_nodes = list(network_stats_df_week3['agg_sentiment_polarity']*2) #Multiply by 2 to see more contrast in colors\n",
    "mutual_follow_lol = network_df[['source_screen_name','destination_screen_name']][network_df['has_mutual_following']==True].values.tolist()\n",
    "mutual_follow_edges = []\n",
    "for mfe in mutual_follow_lol:\n",
    "    mutual_follow_edges.append((mfe[0],mfe[1]))\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(mutual_follow_edges)\n",
    "plt.figure(figsize=(75,75))\n",
    "pos = nx.spring_layout(G, k=1.5*1/np.sqrt(len(G.nodes())), iterations=25)\n",
    "nx.draw(G,pos=pos,with_labels = True,font_size=50,node_size=size_of_nodes,node_color=color_of_nodes,\n",
    "        cmap=plt.get_cmap('YlOrBr'),seed=10)\n",
    "plt.title('''Popular Social Network Graph of people who have tweeted #Brexit on 3rd week of April\n",
    "             Size of nodes indicates number of retweets\n",
    "             Edges indicates mutual following relation\n",
    "             Color of nodes indicates sentiment''',fontsize=100)\n",
    "plt.savefig(\"network_graph_week3_retweet_thresh_\"+str(retweet_threshold)+\".jpeg\") #save as jpeg\n",
    "plt.show() #display\n",
    "#Intensive color means the sentiment for #Brexit is positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network for week 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the mutual relation using networkX library\n",
    "nodes = list(network_stats_df_week4['screen_name'])\n",
    "size_of_nodes = list(network_stats_df_week4['total_retweet_count'])\n",
    "color_of_nodes = list(network_stats_df_week4['agg_sentiment_polarity']*2) #Multiply by 2 to see more contrast in colors\n",
    "mutual_follow_lol = network_df[['source_screen_name','destination_screen_name']][network_df['has_mutual_following']==True].values.tolist()\n",
    "mutual_follow_edges = []\n",
    "for mfe in mutual_follow_lol:\n",
    "    mutual_follow_edges.append((mfe[0],mfe[1]))\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(mutual_follow_edges)\n",
    "plt.figure(figsize=(75,75))\n",
    "pos = nx.spring_layout(G, k=1.5*1/np.sqrt(len(G.nodes())), iterations=25)\n",
    "nx.draw(G,pos=pos,with_labels = True,font_size=50,node_size=size_of_nodes,node_color=color_of_nodes,\n",
    "        cmap=plt.get_cmap('YlOrBr'),seed=10)\n",
    "plt.title('''Popular Social Network Graph of people who have tweeted #Brexit on 4th week of April\n",
    "             Size of nodes indicates number of retweets\n",
    "             Edges indicates mutual following relation\n",
    "             Color of nodes indicates sentiment''',fontsize=100)\n",
    "plt.savefig(\"network_graph_week4_retweet_thresh_\"+str(retweet_threshold)+\".jpeg\") #save as jpeg\n",
    "plt.show() #display\n",
    "#Intensive color means the sentiment for #Brexit is positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Centrality Plot\n",
    "df_centrality = network_stats_df_week1.copy()\n",
    "df_centrality.head()\n",
    "df_centrality['color'] = 'gold'\n",
    "df_centrality['total_retweet_count'] = 500\n",
    "df_centrality.at[226,'total_retweet_count'] = 10000\n",
    "df_centrality.at[164,'total_retweet_count'] = 10000\n",
    "df_centrality.at[122,'total_retweet_count'] = 10000\n",
    "#df_centrality.at[121,'total_retweet_count'] = 10000\n",
    "\n",
    "df_centrality.at[226,'color'] = 'red'  #Degree Centrality & Eigenvector centrality\n",
    "df_centrality.at[164,'color'] = 'lime'  #Betweenness Centrality\n",
    "df_centrality.at[122,'color'] = 'magenta'  #Closeness Centrality\n",
    "#df_centrality.at[121,'color'] = 'cyan'  #Follower/following ratio\n",
    "\n",
    "color_of_nodes = list(df_centrality['color']) #Multiply by 2 to see more contrast in colors\n",
    "nodes = list(df_centrality['screen_name'])\n",
    "size_of_nodes = list(df_centrality['total_retweet_count'])\n",
    "mutual_follow_lol = network_df[['source_screen_name','destination_screen_name']][network_df['has_mutual_following']==True].values.tolist()\n",
    "mutual_follow_edges = []\n",
    "for mfe in mutual_follow_lol:\n",
    "    mutual_follow_edges.append((mfe[0],mfe[1]))\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(mutual_follow_edges)\n",
    "plt.figure(figsize=(75,75))\n",
    "pos = nx.spring_layout(G, k=3*1/np.sqrt(len(G.nodes())), iterations=25)\n",
    "nx.draw(G,pos=pos,with_labels = True,font_size=50,node_size=size_of_nodes,node_color=color_of_nodes,\n",
    "        cmap=plt.get_cmap('YlOrBr'),seed=10)\n",
    "plt.title('''Centrality Plots''',fontsize=100)\n",
    "plt.savefig(\"centrality.jpeg\") #save as jpeg\n",
    "plt.show() #display\n",
    "#Intensive color means the sentiment for #Brexit is positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Algorithms (Undirected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL NODE CONNECTIVITY\n",
    "#Compute node connectivity between all pairs of nodes. (This call takes time)\n",
    "#network_all_node_pair_connectivity = approx.all_pairs_node_connectivity(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOCAL NODE CONNECTIVITY\n",
    "#Give a source node & a target node to check if there is a connectivity between them\n",
    "from networkx.algorithms import approximation as approx\n",
    "network_local_node_connectivity = approx.local_node_connectivity(G,'SkyNews','BBCPolitics')\n",
    "network_local_node_connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NODE CONNECTIVITY\n",
    "network_node_connectivity = approx.node_connectivity(G)\n",
    "network_node_connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BIPARTITE CLUSTERING\n",
    "#Compute a bipartite clustering coefficient for nodes.\n",
    "#nx.algorithms.bipartite.clustering(G) #The graph is not bipartite, so it produces error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLUSTER TRIANGLES\n",
    "#Finds the number of triangles that include a node as one vertex.\n",
    "network_clustering_triangles = nx.triangles(G)\n",
    "sorted(network_clustering_triangles.items(), key=lambda x: x[1],reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLUSTER TRANSITIVITY\n",
    "#Compute graph transitivity, the fraction of all possible triangles present in G.\n",
    "network_clustering_transitivity = nx.transitivity(G)\n",
    "network_clustering_transitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQUARE CLUSTERING\n",
    "#Compute the squares clustering coefficient for nodes.\n",
    "network_square_clustering = nx.square_clustering(G)\n",
    "sorted(network_square_clustering.items(), key=lambda x: x[1],reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLUSTERING\n",
    "#Compute the clustering coefficient for nodes.\n",
    "network_clustering = nx.clustering(G)\n",
    "sorted(network_clustering.items(), key=lambda x: x[1],reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AVERAGE CLUSTERING\n",
    "#Estimates the average clustering coefficient of G\n",
    "network_average_clustering = nx.average_clustering(G)\n",
    "network_average_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GENERALIZED DEGREE\n",
    "#Compute the generalized degree for nodes.\n",
    "#For each node, the generalized degree shows how many edges of given triangle multiplicity the node is connected to.\n",
    "network_generalized_degree = nx.generalized_degree(G)\n",
    "print(network_generalized_degree['BBCPolitics'])\n",
    "print(network_generalized_degree['AJBillingham4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEGREE CENTRALITY\n",
    "network_degree_centrality = nx.degree_centrality(G)\n",
    "sorted(network_degree_centrality.items(), key=lambda x: x[1],reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EIGENVECTOR CENTRALITY\n",
    "network_eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "sorted(network_eigenvector_centrality.items(), key=lambda x: x[1],reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLOSENESS CENTRALITY\n",
    "network_closeness_centrality = nx.closeness_centrality(G)\n",
    "sorted(network_closeness_centrality.items(), key=lambda x: x[1],reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BETWEENNESS CENTRALITY\n",
    "network_betweenness_centrality = nx.betweenness_centrality(G)\n",
    "sorted(network_betweenness_centrality.items(), key=lambda x: x[1],reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDGE BETWEENNESS CENTRALITY\n",
    "network_edge_betweenness_centrality = nx.edge_betweenness_centrality(G)\n",
    "sorted(network_edge_betweenness_centrality.items(), key=lambda x: x[1],reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communicability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return communicability between all pairs of nodes in G.\n",
    "#The communicability between pairs of nodes in G is the sum of closed walks of different lengths starting at node u and \n",
    "#ending at node v.\n",
    "network_communicatability = nx.communicability(G)\n",
    "#network_communicatability['BBCPolitics'] #Shows the communicability of BBCPolitics with all other nodes\n",
    "sorted(network_communicatability['BBCPolitics'].items(), key=lambda x: x[1],reverse=True)[:10] #Top 10 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMMUNICABILITY BETWEENNESS CENTRALITY\n",
    "#Communicability() - Communicability between pairs of nodes in G.\n",
    "#communicability_betweenness_centrality() - Communicability betweeness centrality for each node in G.\n",
    "network_communicatability_bw_centrality = nx.communicability_betweenness_centrality(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PAGERANK\n",
    "#PageRank analysis of graph structure.\n",
    "#PageRank computes a ranking of the nodes in the graph G based on the structure of the incoming links. \n",
    "network_pagerank = nx.pagerank(G)\n",
    "sorted(network_pagerank.items(), key=lambda x: x[1],reverse=True)[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HITS\n",
    "#Return HITS hubs and authorities values for nodes.\n",
    "#The HITS algorithm computes two numbers for a node. Authorities estimates the node value based on the incoming links. \n",
    "    #Hubs estimates the node value based on outgoing links.\n",
    "network_hubs,network_authorities = nx.hits(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MINIMUM SPANNING TREE\n",
    "network_min_spanning_tree = nx.minimum_spanning_tree(G)\n",
    "sorted(network_min_spanning_tree.edges(data=True))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAXIMUM SPANNING TREE\n",
    "network_max_spanning_tree = nx.maximum_spanning_tree(G)\n",
    "sorted(network_max_spanning_tree.edges(data=True))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MINIMUM SPANNING EDGES\n",
    "network_min_spanning_edges = nx.minimum_spanning_edges(G)\n",
    "sorted(list(network_min_spanning_edges))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAXIMUM SPANNING EDGES\n",
    "network_max_spanning_edges = nx.maximum_spanning_edges(G)\n",
    "sorted(list(network_max_spanning_edges))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vitality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLOSENESS VITALITY\n",
    "#Returns the closeness vitality for nodes in the graph.\n",
    "#The closeness vitality of a node is the change in the sum of distances between all node pairs when excluding that node.\n",
    "network_vitality = nx.closeness_vitality(G)#Requires closely connected graph, else returns nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiener index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the Wiener index of the given graph.\n",
    "#The Wiener index of a graph is the sum of the shortest-path distances between each pair of reachable nodes. \n",
    "#For pairs of nodes in undirected graphs, only one orientation of the pair is counted.\n",
    "nx.wiener_index(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KERNIGHAN–LIN (BIPARTITIAN) COMMUNITY\n",
    "network_community_kl = nx.community.kernighan_lin.kernighan_lin_bisection(G) \n",
    "#It may give better results if we give the weights of edges\n",
    "len(network_community_kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GREEDY MODULARITY COMMUNITY\n",
    "#Find communities in graph using Clauset-Newman-Moore greedy modularity maximization. \n",
    "#This method currently supports the Graph class and does not consider edge weights.\n",
    "#Greedy modularity maximization begins with each node in its own community and joins the pair of communities that most \n",
    "#increases modularity until no such pair exists.\n",
    "network_community_modularity = nx.community.greedy_modularity_communities(G)\n",
    "len(network_community_modularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-CLIQUE COMMUNITY DETECTION\n",
    "#Find k-clique communities in graph using the percolation method.\n",
    "#A k-clique community is the union of all cliques of size k that can be reached through adjacent (sharing k-1 nodes) k-cliques.\n",
    "network_community_k_clique = nx.community.k_clique_communities(G,5) #Set k = 5\n",
    "network_community_k_clique = list(network_community_k_clique)\n",
    "len(network_community_k_clique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LABEL PROPAGATION COMMUNITY\n",
    "#Generates community sets determined by label propagation\n",
    "network_community_lpa = nx.community.label_propagation.label_propagation_communities(G)\n",
    "network_community_lpa = list(network_community_lpa)[::-1]\n",
    "len(network_community_lpa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIRVAN–NEWMAN COMMUNITY\n",
    "#Partitions via centrality measures\n",
    "network_community_gn = nx.community.centrality.girvan_newman(G)\n",
    "network_community_gn = list(tuple(set(c) for c in next(network_community_gn)))\n",
    "len(network_community_gn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOUVAIN METHOD\n",
    "#!pip install python-louvain\n",
    "import community\n",
    "partition = community.best_partition(G)\n",
    "network_community_louvain = []\n",
    "for i in range(len(set(partition.values()))):\n",
    "    community_members = []\n",
    "    for key, value in partition.items():\n",
    "        if value == i:\n",
    "            community_members.append(key)\n",
    "    network_community_louvain.append(set(community_members))\n",
    "    \n",
    "len(network_community_louvain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_df = network_stats_df_week1.copy()\n",
    "community_df['kernighanLin_ID'] = '' #Bipartitian\n",
    "community_df['kernighanLin_size'] = 500\n",
    "community_df['GModularity_ID'] = ''\n",
    "community_df['GModularity_size'] = 500\n",
    "community_df['kClique_ID'] = ''\n",
    "community_df['kClique_size'] = 500\n",
    "community_df['labelProp_ID'] = ''\n",
    "community_df['labelProp_size'] = 500\n",
    "community_df['girvanNew_ID'] = ''\n",
    "community_df['girvanNew_size'] = 500\n",
    "community_df['louvain_ID'] = ''\n",
    "community_df['louvain_size'] = 500\n",
    "for index,row in community_df.iterrows():\n",
    "    for h in range(len(network_community_kl)):\n",
    "        if row['screen_name'] in network_community_kl[h]:\n",
    "            community_df.at[index,'kernighanLin_ID'] = h\n",
    "            community_df.at[index,'kernighanLin_size'] = 2500\n",
    "    for i in range(len(network_community_modularity)):\n",
    "        if len(network_community_modularity[i]) >= 3:  #Only consider communities having at least 3 members\n",
    "            if row['screen_name'] in network_community_modularity[i]:\n",
    "                community_df.at[index,'GModularity_ID'] = i\n",
    "                community_df.at[index,'GModularity_size'] = 2500\n",
    "    for j in range(len(network_community_k_clique)):\n",
    "        if row['screen_name'] in network_community_k_clique[j]:\n",
    "            community_df.at[index,'kClique_ID'] = j\n",
    "            community_df.at[index,'kClique_size'] = 2500\n",
    "    for k in range(len(network_community_lpa)):\n",
    "        if len(network_community_lpa[k]) >= 3:  #Only consider communities having at least 3 members\n",
    "            if row['screen_name'] in network_community_lpa[k]:\n",
    "                community_df.at[index,'labelProp_ID'] = k\n",
    "                community_df.at[index,'labelProp_size'] = 2500\n",
    "    for l in range(len(network_community_gn)):\n",
    "        if len(network_community_gn[l]) >= 3:  #Only consider communities having at least 3 members\n",
    "            if row['screen_name'] in network_community_gn[l]:\n",
    "                community_df.at[index,'girvanNew_ID'] = l\n",
    "                community_df.at[index,'girvanNew_size'] = 2500\n",
    "    for m in range(len(network_community_louvain)):\n",
    "        if len(network_community_louvain[m]) >= 3:  #Only consider communities having at least 3 members\n",
    "            if row['screen_name'] in network_community_louvain[m]:\n",
    "                community_df.at[index,'louvain_ID'] = m\n",
    "                community_df.at[index,'louvain_size'] = 2500\n",
    "print(community_df.shape)\n",
    "community_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = ['blue','red','green','brown','orange','crimson','cyan','pink','darkslategray','darkgreen','olive']\n",
    "community_df['kernighanLin_ID'] = community_df['kernighanLin_ID'].astype(str)\n",
    "community_df['kClique_ID'] = community_df['kClique_ID'].astype(str)\n",
    "community_df['GModularity_ID'] = community_df['GModularity_ID'].astype(str)\n",
    "community_df['labelProp_ID'] = community_df['labelProp_ID'].astype(str)\n",
    "community_df['girvanNew_ID'] = community_df['girvanNew_ID'].astype(str)\n",
    "community_df['louvain_ID'] = community_df['louvain_ID'].astype(str)\n",
    "for c in range(len(color_list)):\n",
    "    community_df['kernighanLin_ID'] = community_df['kernighanLin_ID'].replace(str(c),color_list[c])\n",
    "    community_df['kClique_ID'] = community_df['kClique_ID'].replace(str(c),color_list[c])\n",
    "    community_df['GModularity_ID'] = community_df['GModularity_ID'].replace(str(c),color_list[c])\n",
    "    community_df['labelProp_ID'] = community_df['labelProp_ID'].replace(str(c),color_list[c])\n",
    "    community_df['girvanNew_ID'] = community_df['girvanNew_ID'].replace(str(c),color_list[c])\n",
    "    community_df['louvain_ID'] = community_df['louvain_ID'].replace(str(c),color_list[c])\n",
    "community_df['kernighanLin_ID'] = community_df['kernighanLin_ID'].replace('','gold')\n",
    "community_df['kClique_ID'] = community_df['kClique_ID'].replace('','gold')\n",
    "community_df['GModularity_ID'] = community_df['GModularity_ID'].replace('','gold')\n",
    "community_df['labelProp_ID'] = community_df['labelProp_ID'].replace('','gold')\n",
    "community_df['girvanNew_ID'] = community_df['girvanNew_ID'].replace('','gold')\n",
    "community_df['louvain_ID'] = community_df['louvain_ID'].replace('','gold')\n",
    "community_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(community_df['screen_name'])\n",
    "size_of_nodes = list(community_df['kernighanLin_size'])\n",
    "color_of_nodes = list(community_df['kernighanLin_ID']) #Multiply by 2 to see more contrast in colors\n",
    "mutual_follow_lol = network_df[['source_screen_name','destination_screen_name']][network_df['has_mutual_following']==True].values.tolist()\n",
    "mutual_follow_edges = []\n",
    "for mfe in mutual_follow_lol:\n",
    "    mutual_follow_edges.append((mfe[0],mfe[1]))\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(mutual_follow_edges)\n",
    "plt.figure(figsize=(75,75))\n",
    "pos = nx.spring_layout(G, k=2*1/np.sqrt(len(G.nodes())), iterations=30)\n",
    "nx.draw(G,pos=pos,with_labels = True,font_size=50,node_size=size_of_nodes,node_color=color_of_nodes,seed=1)\n",
    "plt.title('''Popular Social Network Graph of people who have tweeted #Brexit on each week of April\n",
    "             Kernighan Lin Community''',fontsize=100)\n",
    "plt.show() #display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(community_df['screen_name'])\n",
    "size_of_nodes = list(community_df['kClique_size'])\n",
    "color_of_nodes = list(community_df['kClique_ID']) #Multiply by 2 to see more contrast in colors\n",
    "mutual_follow_lol = network_df[['source_screen_name','destination_screen_name']][network_df['has_mutual_following']==True].values.tolist()\n",
    "mutual_follow_edges = []\n",
    "for mfe in mutual_follow_lol:\n",
    "    mutual_follow_edges.append((mfe[0],mfe[1]))\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(mutual_follow_edges)\n",
    "plt.figure(figsize=(75,75))\n",
    "pos = nx.spring_layout(G, k=2*1/np.sqrt(len(G.nodes())), iterations=30)\n",
    "nx.draw(G,pos=pos,with_labels = True,font_size=50,node_size=size_of_nodes,node_color=color_of_nodes,seed=1)\n",
    "plt.title('''Popular Social Network Graph of people who have tweeted #Brexit on each week of April\n",
    "             K-Clique Community''',fontsize=100)\n",
    "plt.show() #display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(community_df['screen_name'])\n",
    "size_of_nodes = list(community_df['GModularity_size'])\n",
    "color_of_nodes = list(community_df['GModularity_ID']) #Multiply by 2 to see more contrast in colors\n",
    "mutual_follow_lol = network_df[['source_screen_name','destination_screen_name']][network_df['has_mutual_following']==True].values.tolist()\n",
    "mutual_follow_edges = []\n",
    "for mfe in mutual_follow_lol:\n",
    "    mutual_follow_edges.append((mfe[0],mfe[1]))\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(mutual_follow_edges)\n",
    "plt.figure(figsize=(75,75))\n",
    "pos = nx.spring_layout(G, k=1.5*1/np.sqrt(len(G.nodes())), iterations=25)\n",
    "nx.draw(G,pos=pos,with_labels = True,font_size=50,node_size=size_of_nodes,node_color=color_of_nodes,seed=10)\n",
    "plt.title('''Popular Social Network Graph of people who have tweeted #Brexit on each week of April\n",
    "             Greedy Modularity Community''',fontsize=100)\n",
    "plt.savefig('greedy_modularity.png')\n",
    "plt.show() #display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(community_df['screen_name'])\n",
    "size_of_nodes = list(community_df['labelProp_size'])\n",
    "color_of_nodes = list(community_df['labelProp_ID']) #Multiply by 2 to see more contrast in colors\n",
    "mutual_follow_lol = network_df[['source_screen_name','destination_screen_name']][network_df['has_mutual_following']==True].values.tolist()\n",
    "mutual_follow_edges = []\n",
    "for mfe in mutual_follow_lol:\n",
    "    mutual_follow_edges.append((mfe[0],mfe[1]))\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(mutual_follow_edges)\n",
    "plt.figure(figsize=(75,75))\n",
    "pos = nx.spring_layout(G, k=1.5*1/np.sqrt(len(G.nodes())), iterations=25)\n",
    "nx.draw(G,pos=pos,with_labels = True,font_size=50,node_size=size_of_nodes,node_color=color_of_nodes,seed=10)\n",
    "plt.title('''Popular Social Network Graph of people who have tweeted #Brexit on each week of April\n",
    "             Label Propagation Community''',fontsize=100)\n",
    "plt.show() #display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(community_df['screen_name'])\n",
    "size_of_nodes = list(community_df['girvanNew_size'])\n",
    "color_of_nodes = list(community_df['girvanNew_ID']) #Multiply by 2 to see more contrast in colors\n",
    "mutual_follow_lol = network_df[['source_screen_name','destination_screen_name']][network_df['has_mutual_following']==True].values.tolist()\n",
    "mutual_follow_edges = []\n",
    "for mfe in mutual_follow_lol:\n",
    "    mutual_follow_edges.append((mfe[0],mfe[1]))\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(mutual_follow_edges)\n",
    "plt.figure(figsize=(75,75))\n",
    "pos = nx.spring_layout(G, k=1.5*1/np.sqrt(len(G.nodes())), iterations=25)\n",
    "nx.draw(G,pos=pos,with_labels = True,font_size=50,node_size=size_of_nodes,node_color=color_of_nodes,seed=10)\n",
    "plt.title('''Popular Social Network Graph of people who have tweeted #Brexit on each week of April\n",
    "             Girvan-Newman Community''',fontsize=100)\n",
    "plt.show() #display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(community_df['screen_name'])\n",
    "size_of_nodes = list(community_df['louvain_size'])\n",
    "color_of_nodes = list(community_df['louvain_ID']) #Multiply by 2 to see more contrast in colors\n",
    "mutual_follow_lol = network_df[['source_screen_name','destination_screen_name']][network_df['has_mutual_following']==True].values.tolist()\n",
    "mutual_follow_edges = []\n",
    "for mfe in mutual_follow_lol:\n",
    "    mutual_follow_edges.append((mfe[0],mfe[1]))\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(mutual_follow_edges)\n",
    "plt.figure(figsize=(75,75))\n",
    "pos = nx.spring_layout(G, k=1.5*1/np.sqrt(len(G.nodes())), iterations=25)\n",
    "nx.draw(G,pos=pos,with_labels = True,font_size=50,node_size=size_of_nodes,node_color=color_of_nodes,seed=10)\n",
    "plt.title('''Popular Social Network Graph of people who have tweeted #Brexit on each week of April\n",
    "             Louvain Method''',fontsize=100)\n",
    "plt.show() #display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEIDEN METHOD\n",
    "#!pip install leidenalg\n",
    "import leidenalg\n",
    "#!pip install igraph\n",
    "import igraph as ig\n",
    "nx.write_graphml(G,'graph.graphml')\n",
    "Gix = ig.read('graph.graphml',format=\"graphml\")\n",
    "network_community_leiden = leidenalg.find_partition(Gix, leidenalg.ModularityVertexPartition);\n",
    "network_community_leiden = list(network_community_leiden)\n",
    "\n",
    "community_df = network_stats_df_week1.copy()\n",
    "community_df['leiden_ID'] = '' #Bipartitian\n",
    "community_df['leiden_size'] = 500\n",
    "\n",
    "group_id = 0\n",
    "for i in range(len(network_community_leiden)):\n",
    "    if len(network_community_leiden[i]) >= 3:  #Only consider communities having at least 3 members\n",
    "        for index,row in community_df.iterrows():\n",
    "              if index in network_community_leiden[i]:\n",
    "                    community_df.at[index,'leiden_ID'] = group_id\n",
    "                    community_df.at[index,'leiden_size'] = 2500\n",
    "            group_id = group_id+1\n",
    "                \n",
    "color_list = ['blue','red','green','brown','orange','crimson','cyan','pink','darkslategray','darkgreen','olive']\n",
    "community_df['leiden_ID'] = community_df['leiden_ID'].astype(str)\n",
    "for c in range(len(color_list)):\n",
    "    community_df['leiden_ID'] = community_df['leiden_ID'].replace(str(c),color_list[c])\n",
    "community_df['leiden_ID'] = community_df['leiden_ID'].replace('','gold')\n",
    "community_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(community_df['screen_name'])\n",
    "size_of_nodes = list(community_df['leiden_size'])\n",
    "color_of_nodes = list(community_df['leiden_ID']) #Multiply by 2 to see more contrast in colors\n",
    "mutual_follow_lol = network_df[['source_screen_name','destination_screen_name']][network_df['has_mutual_following']==True].values.tolist()\n",
    "mutual_follow_edges = []\n",
    "for mfe in mutual_follow_lol:\n",
    "    mutual_follow_edges.append((mfe[0],mfe[1]))\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(mutual_follow_edges)\n",
    "plt.figure(figsize=(75,75))\n",
    "pos = nx.spring_layout(G, k=1.5*1/np.sqrt(len(G.nodes())), iterations=25)\n",
    "nx.draw(G,pos=pos,with_labels = True,font_size=50,node_size=size_of_nodes,node_color=color_of_nodes,seed=10)\n",
    "plt.title('''Popusalar Social Network Graph of people who have tweeted #Brexit on each week of April\n",
    "             Leiden Method''',fontsize=100)\n",
    "plt.savefig('leiden_alg.png')\n",
    "plt.show() #display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of Community algorithms based on Sentiments about Brexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_week_sentiment_df = pd.concat((network_stats_df_week1[['screen_name','agg_sentiment_polarity']].rename(columns={'agg_sentiment_polarity':'week1_sentiment'}),\n",
    "                          network_stats_df_week2[['screen_name','agg_sentiment_polarity']].rename(columns={'agg_sentiment_polarity':'week2_sentiment','screen_name':'screen_name_2'}),\n",
    "                          network_stats_df_week3[['screen_name','agg_sentiment_polarity']].rename(columns={'agg_sentiment_polarity':'week3_sentiment','screen_name':'screen_name_3'}),\n",
    "                          network_stats_df_week4[['screen_name','agg_sentiment_polarity']].rename(columns={'agg_sentiment_polarity':'week4_sentiment','screen_name':'screen_name_4'})\n",
    "                          ),axis=1)\n",
    "all_week_sentiment_df = all_week_sentiment_df.drop(['screen_name_2','screen_name_3','screen_name_4'],axis=1)\n",
    "print(all_week_sentiment_df.shape)\n",
    "all_week_sentiment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df = pd.read_csv('nodes_list_retweet_thresh_10.csv').drop(columns=['Unnamed: 0'],axis=1).rename(columns={'Twitter_users':'screen_name'})\n",
    "nw_community_df = pd.merge(tweet_df[['screen_name','retweet_count']],node_df,on='screen_name',how='inner')\n",
    "nw_community_df = nw_community_df.groupby('screen_name').sum()\n",
    "nw_community_df.reset_index(inplace=True)\n",
    "nw_community_df['retweet_count'] = nw_community_df['retweet_count'].astype(float)\n",
    "nw_community_df['kernighanLin_ID'] = '' #Bipartitian\n",
    "nw_community_df['GModularity_ID'] = ''\n",
    "nw_community_df['kClique_ID'] = ''\n",
    "nw_community_df['labelProp_ID'] = ''\n",
    "nw_community_df['girvanNew_ID'] = ''\n",
    "nw_community_df['louvain_ID'] = ''\n",
    "for index,row in nw_community_df.iterrows():\n",
    "    for h in range(len(network_community_kl)):\n",
    "        if row['screen_name'] in network_community_kl[h]:\n",
    "            nw_community_df.at[index,'kernighanLin_ID'] = h\n",
    "    for i in range(len(network_community_modularity)):\n",
    "        if row['screen_name'] in network_community_modularity[i]:\n",
    "            nw_community_df.at[index,'GModularity_ID'] = i\n",
    "    for j in range(len(network_community_k_clique)):\n",
    "        if row['screen_name'] in network_community_k_clique[j]:\n",
    "            nw_community_df.at[index,'kClique_ID'] = j\n",
    "    for k in range(len(network_community_lpa)):\n",
    "        if row['screen_name'] in network_community_lpa[k]:\n",
    "            nw_community_df.at[index,'labelProp_ID'] = k\n",
    "    for l in range(len(network_community_gn)):\n",
    "        if row['screen_name'] in network_community_gn[l]:\n",
    "            nw_community_df.at[index,'girvanNew_ID'] = l\n",
    "    for m in range(len(network_community_louvain)):\n",
    "        if row['screen_name'] in network_community_louvain[m]:\n",
    "            nw_community_df.at[index,'louvain_ID'] = m\n",
    "            \n",
    "nw_community_df = pd.merge(nw_community_df,pd.read_csv('leiden_df.csv')[['screen_name','leiden_ID']],on='screen_name',how='inner')\n",
    "nw_community_df = nw_community_df.drop(['retweet_count'],axis=1)\n",
    "\n",
    "community_validation_df = pd.merge(nw_community_df,all_week_sentiment_df,on='screen_name',how='inner')\n",
    "print(community_validation_df.shape)\n",
    "community_validation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "community_validation_df[['week1_sentiment','week2_sentiment','week3_sentiment','week4_sentiment']] = scaler.fit_transform(community_validation_df[['week1_sentiment','week2_sentiment','week3_sentiment','week4_sentiment']])\n",
    "community_validation_df.head()\n",
    "\n",
    "#OR\n",
    "\n",
    "#community_validation_df[['week1_sentiment','week2_sentiment','week3_sentiment','week4_sentiment']] = community_validation_df[['week1_sentiment','week2_sentiment','week3_sentiment','week4_sentiment']]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_cols = ['mean_sentiment_CV']\n",
    "community_sent_cv_df = pd.DataFrame(columns = comm_cols)\n",
    "community_alg = {'Kernighan-Lin':'kernighanLin_ID','Greedy Modularity':'GModularity_ID','k-Clique':'kClique_ID',\n",
    "                 'Label-Propagation':'labelProp_ID','Girvan-Newman':'girvanNew_ID','Louvain Method':'louvain_ID',\n",
    "                 'Leiden Method':'leiden_ID'}\n",
    "for key,value in community_alg.items():\n",
    "    df = community_validation_df[[value,'week1_sentiment','week2_sentiment','week3_sentiment','week4_sentiment']].copy()\n",
    "    mean_cv = np.mean(np.mean(df.groupby(value).std()/df.groupby(value).mean()))\n",
    "    df = pd.DataFrame([mean_cv],columns=comm_cols,index=[key])\n",
    "    community_sent_cv_df = community_sent_cv_df.append(df)\n",
    "community_sent_cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_cols = ['mean_sentiment_CV_week1','mean_sentiment_CV_week2','mean_sentiment_CV_week3','mean_sentiment_CV_week4']\n",
    "community_sent_cv_df = pd.DataFrame(columns = comm_cols)\n",
    "community_alg = {'Kernighan-Lin':'kernighanLin_ID','Greedy Modularity':'GModularity_ID','k-Clique':'kClique_ID',\n",
    "                 'Label-Propagation':'labelProp_ID','Girvan-Newman':'girvanNew_ID','Louvain Method':'louvain_ID',\n",
    "                 'Leiden Method':'leiden_ID'}\n",
    "for key,value in community_alg.items():\n",
    "    df = community_validation_df[[value,'week1_sentiment','week2_sentiment','week3_sentiment','week4_sentiment']].copy()\n",
    "    mean_cv = np.mean(df.groupby(value).std()/df.groupby(value).mean())\n",
    "    df = pd.DataFrame([[mean_cv[0],mean_cv[1],mean_cv[2],mean_cv[3]]],\n",
    "                      columns=comm_cols,index=[key])\n",
    "    community_sent_cv_df = community_sent_cv_df.append(df)\n",
    "community_sent_cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = community_validation_df[['labelProp_ID','week1_sentiment','week2_sentiment','week3_sentiment','week4_sentiment']].copy()\n",
    "value = df.groupby('labelProp_ID').std()/df.groupby('labelProp_ID').mean()\n",
    "np.mean(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(value)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in community_alg.items():\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JS Visualization Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undirected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(js_community_df['leiden_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df = pd.read_csv('nodes_list_retweet_thresh_10.csv').drop(columns=['Unnamed: 0'],axis=1).rename(columns={'Twitter_users':'screen_name'})\n",
    "js_community_df = pd.merge(tweet_df[['screen_name','retweet_count']],node_df,on='screen_name',how='inner')\n",
    "js_community_df = js_community_df.groupby('screen_name').sum()\n",
    "js_community_df.reset_index(inplace=True)\n",
    "js_community_df['retweet_count'] = js_community_df['retweet_count'].astype(float)\n",
    "js_community_df['kernighanLin_ID'] = '' #Bipartitian\n",
    "js_community_df['GModularity_ID'] = ''\n",
    "js_community_df['kClique_ID'] = ''\n",
    "js_community_df['labelProp_ID'] = ''\n",
    "js_community_df['girvanNew_ID'] = ''\n",
    "js_community_df['louvain_ID'] = ''\n",
    "for index,row in js_community_df.iterrows():\n",
    "    for h in range(len(network_community_kl)):\n",
    "        if row['screen_name'] in network_community_kl[h]:\n",
    "            js_community_df.at[index,'kernighanLin_ID'] = h\n",
    "    for i in range(len(network_community_modularity)):\n",
    "        if row['screen_name'] in network_community_modularity[i]:\n",
    "            js_community_df.at[index,'GModularity_ID'] = i\n",
    "    for j in range(len(network_community_k_clique)):\n",
    "        if row['screen_name'] in network_community_k_clique[j]:\n",
    "            js_community_df.at[index,'kClique_ID'] = j\n",
    "    for k in range(len(network_community_lpa)):\n",
    "        if row['screen_name'] in network_community_lpa[k]:\n",
    "            js_community_df.at[index,'labelProp_ID'] = k\n",
    "    for l in range(len(network_community_gn)):\n",
    "        if row['screen_name'] in network_community_gn[l]:\n",
    "            js_community_df.at[index,'girvanNew_ID'] = l\n",
    "    for m in range(len(network_community_louvain)):\n",
    "        if row['screen_name'] in network_community_louvain[m]:\n",
    "            js_community_df.at[index,'louvain_ID'] = m\n",
    "            \n",
    "js_community_df = pd.merge(js_community_df,pd.read_csv('leiden_df.csv')[['screen_name','leiden_ID']],on='screen_name',how='inner')\n",
    "js_community_df = js_community_df.drop(['retweet_count'],axis=1)\n",
    "print(js_community_df.shape)\n",
    "js_community_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_centrality_df = pd.DataFrame(columns=['screen_name','degree_centrality'])\n",
    "for key,value in network_degree_centrality.items():\n",
    "    degree_centrality_df = degree_centrality_df.append(pd.DataFrame([[key,value]],columns=['screen_name','degree_centrality']))\n",
    "degree_centrality_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "eigenvector_centrality_df = pd.DataFrame(columns=['screen_name','eigenvector_centrality'])\n",
    "for key,value in network_eigenvector_centrality.items():\n",
    "    eigenvector_centrality_df = eigenvector_centrality_df.append(pd.DataFrame([[key,value]],columns=['screen_name','eigenvector_centrality']))\n",
    "eigenvector_centrality_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "closeness_centrality_df = pd.DataFrame(columns=['screen_name','closeness_centrality'])\n",
    "for key,value in network_closeness_centrality.items():\n",
    "    closeness_centrality_df = closeness_centrality_df.append(pd.DataFrame([[key,value]],columns=['screen_name','closeness_centrality']))\n",
    "closeness_centrality_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "betweenness_centrality_df = pd.DataFrame(columns=['screen_name','betweenness_centrality'])\n",
    "for key,value in network_betweenness_centrality.items():\n",
    "    betweenness_centrality_df = betweenness_centrality_df.append(pd.DataFrame([[key,value]],columns=['screen_name','betweenness_centrality']))\n",
    "betweenness_centrality_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "temp_df = pd.merge(degree_centrality_df,eigenvector_centrality_df,on='screen_name',how='inner')\n",
    "temp_df1 = pd.merge(temp_df,closeness_centrality_df,on='screen_name',how='inner')\n",
    "centrality_df = pd.merge(temp_df1,betweenness_centrality_df,on='screen_name',how='inner')\n",
    "print(centrality_df.shape)\n",
    "centrality_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undirected_comm_centrality = pd.merge(js_community_df,centrality_df,on='screen_name',how='inner')\n",
    "print(undirected_comm_centrality.shape)\n",
    "undirected_comm_centrality.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_week_nw_dfs = [network_stats_df_week1,network_stats_df_week2,network_stats_df_week3,network_stats_df_week4]\n",
    "undirected_node_json_list_week1 = []\n",
    "undirected_node_json_list_week2 = []\n",
    "undirected_node_json_list_week3 = []\n",
    "undirected_node_json_list_week4 = []\n",
    "all_undirected_week_lists = [undirected_node_json_list_week1,undirected_node_json_list_week2,undirected_node_json_list_week3,\n",
    "                             undirected_node_json_list_week4]\n",
    "for i in range(4):\n",
    "    week_df = all_week_nw_dfs[i]\n",
    "    node_df_i = pd.merge(week_df,undirected_comm_centrality,on='screen_name',how='inner')\n",
    "    node_df_i['agg_sentiment_polarity'] = round(node_df_i['agg_sentiment_polarity'],6)\n",
    "    node_df_i['degree_centrality'] = round(node_df_i['degree_centrality'],6)\n",
    "    node_df_i['eigenvector_centrality'] = round(node_df_i['eigenvector_centrality'],6)\n",
    "    node_df_i['closeness_centrality'] = round(node_df_i['closeness_centrality'],6)\n",
    "    node_df_i['betweenness_centrality'] = round(node_df_i['betweenness_centrality'],6)\n",
    "    for index,row in node_df_i.iterrows():\n",
    "        new_dict = {'name':row['screen_name'],\n",
    "                    'total_tweet_count':row['total_tweet_count'],\n",
    "                    'total_retweet_count':row['total_retweet_count'],\n",
    "                    'followers_count':row['max_followers_count'],\n",
    "                    'friends_count':row['max_friends_count'],\n",
    "                    'brexit_tweet_sentiment':row['agg_sentiment_polarity'],\n",
    "                    'degree_centrality':row['degree_centrality'],\n",
    "                    'eigenvector_centrality':row['eigenvector_centrality'],\n",
    "                    'closeness_centrality':row['closeness_centrality'],\n",
    "                    'betweenness_centrality':row['betweenness_centrality'],\n",
    "                    'community_kernighanLin':row['kernighanLin_ID'],\n",
    "                    'community_GModularity':row['GModularity_ID'],\n",
    "                    'community_kClique':row['kClique_ID'],\n",
    "                    'community_labelProp':row['labelProp_ID'],\n",
    "                    'community_girvanNew':row['girvanNew_ID'],\n",
    "                    'community_louvain':row['louvain_ID'],\n",
    "                    'community_leiden':row['leiden_ID']}\n",
    "        all_undirected_week_lists[i].append(new_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_following_df = network_df[network_df['has_mutual_following']==True]\n",
    "mutual_following_df.reset_index(drop=True,inplace=True)\n",
    "print(mutual_following_df.shape)\n",
    "mutual_following_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df = pd.read_csv('nodes_list_retweet_thresh_10.csv').drop(columns=['Unnamed: 0'],axis=1).rename(columns={'Twitter_users':'screen_name'})\n",
    "retweet_node_df = pd.merge(tweet_df[['screen_name','retweet_count']],node_df,on='screen_name',how='inner')\n",
    "retweet_node_df = retweet_node_df.groupby('screen_name').sum()\n",
    "retweet_node_df.reset_index(inplace=True)\n",
    "retweet_node_df['retweet_count'] = retweet_node_df['retweet_count'].astype(float)\n",
    "retweet_node_df.head()\n",
    "\n",
    "retweet_edge_df = mutual_following_df[['source_screen_name','destination_screen_name']].copy()\n",
    "retweet_edge_df.reset_index(drop=True,inplace=True)\n",
    "retweet_edge_df['edge_weight'] = 1\n",
    "retweet_edge_df['source_index'] = ''\n",
    "retweet_edge_df['dest_index'] = ''\n",
    "df = retweet_edge_df.copy()\n",
    "for ind,row in df.iterrows():\n",
    "    source_name = row['source_screen_name']\n",
    "    source_index = retweet_node_df[retweet_node_df['screen_name']==source_name].index.values.astype(int)[0]\n",
    "    df.at[ind,'source_index'] = source_index\n",
    "    \n",
    "    dest_name = row['destination_screen_name']\n",
    "    dest_index = retweet_node_df[retweet_node_df['screen_name']==dest_name].index.values.astype(int)[0]\n",
    "    df.at[ind,'dest_index'] = dest_index\n",
    "    \n",
    "undirected_links_json_df = df[['source_index','dest_index','edge_weight']]\n",
    "print(undirected_links_json_df.shape)\n",
    "undirected_links_json_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undirected_links_json_list = []\n",
    "for index,row in undirected_links_json_df.iterrows():\n",
    "    new_dict = {'source':row['source_index'],\n",
    "                'target':row['dest_index'],\n",
    "                'weight':row['edge_weight']}\n",
    "    undirected_links_json_list.append(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_undirected_nodes_week_lists = [undirected_node_json_list_week1,undirected_node_json_list_week2,undirected_node_json_list_week3,\n",
    "                             undirected_node_json_list_week4]\n",
    "for i in range(4):\n",
    "    js_json_data = {'nodes':all_undirected_nodes_week_lists[i],'links':undirected_links_json_list}\n",
    "    with open('undirected_json_data_week_'+str(i+1)+'.json', 'w') as f:\n",
    "        print(js_json_data, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directed Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df = pd.read_csv('Outputs\\mutual_folling_info_retweet_thresh_'+str(retweet_threshold)+'.csv')\n",
    "network_df = network_df.drop(columns=['Unnamed: 0'],axis='columns')\n",
    "\n",
    "source_follow_dest_df = network_df[network_df['source_follow_dest']==True]\n",
    "source_follow_dest_df.reset_index(drop=True,inplace=True)\n",
    "source_follow_dest_df = source_follow_dest_df.drop(columns=['dest_follow_source'],axis='columns')\n",
    "\n",
    "dest_follow_source_df = network_df[network_df['dest_follow_source']==True]\n",
    "dest_follow_source_df.reset_index(drop=True,inplace=True)\n",
    "dest_follow_source_df = dest_follow_source_df.drop(columns=['source_follow_dest'],axis='columns')\n",
    "dest_follow_source_df = dest_follow_source_df.rename(columns={'source_screen_name':'dest_screen_name','destination_screen_name':'source_screen_name','dest_follow_source':'source_follow_dest'})\n",
    "dest_follow_source_df = dest_follow_source_df.rename(columns={'dest_screen_name':'destination_screen_name'})\n",
    "dest_follow_source_df = dest_follow_source_df[['source_screen_name','destination_screen_name','has_mutual_following','source_follow_dest']]\n",
    "\n",
    "following_df = pd.concat([source_follow_dest_df,dest_follow_source_df])\n",
    "following_df.reset_index(drop=True,inplace=True)\n",
    "print(following_df.shape)\n",
    "following_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "nodes_df = pd.read_csv('nodes_list_retweet_thresh_'+str(retweet_threshold)+'.csv')\n",
    "nodes_df = nodes_df.drop(['Unnamed: 0'],axis='columns')\n",
    "\n",
    "tweet_df = pd.read_csv('Outputs/brexit_tweets_april.csv',lineterminator='\\n')\n",
    "tweet_df['mentions'] = tweet_df['mentions\\r'].str.strip()\n",
    "tweet_df['dummy_count'] = 1\n",
    "tweet_df = tweet_df.drop(['Unnamed: 0','id','location','retweeted','mentions\\r'],axis='columns')\n",
    "\n",
    "nodes = list(nodes_df['Twitter_users'])\n",
    "\n",
    "ret_thresh_tweet_df = tweet_df[tweet_df['screen_name'].isin(nodes)]\n",
    "ret_thresh_tweet_df.reset_index(drop=True,inplace=True)\n",
    "ret_thresh_tweet_df['mentions_list'] = ''\n",
    "for index,row in ret_thresh_tweet_df.iterrows():\n",
    "    list_string = row['mentions']\n",
    "    ls_to_list = ast.literal_eval(list_string)\n",
    "    ret_thresh_tweet_df.at[index,'mentions_list'] = ls_to_list\n",
    "    \n",
    "df = ret_thresh_tweet_df[['screen_name','mentions_list']].groupby('screen_name')['mentions_list'].apply(list)\n",
    "df = pd.DataFrame(df)\n",
    "df['screen_name'] = df.index\n",
    "df = df[['screen_name','mentions_list']]\n",
    "df['mentions_flatten_list'] = ''\n",
    "for index,row in df.iterrows():\n",
    "    l = row['mentions_list']\n",
    "    flatten_list = [item[1:] for sublist in l for item in sublist] #Setting item[1:] removes '@' symbol which is required!\n",
    "    df.at[index,'mentions_flatten_list'] = flatten_list\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "following__edge_weight_df = pd.merge(following_df,df[['screen_name','mentions_flatten_list']],\n",
    "                                     left_on='source_screen_name',right_on='screen_name',how='inner')\n",
    "following__edge_weight_df['edge_weight'] = 0.0\n",
    "following__edge_weight_df = following__edge_weight_df.drop(['screen_name'],axis='columns')\n",
    "for index,row in following__edge_weight_df.iterrows():\n",
    "    c_mentions = row['mentions_flatten_list'].count(row['destination_screen_name'])\n",
    "    following__edge_weight_df.at[index,'edge_weight'] = 0.5+c_mentions\n",
    "\n",
    "following__edge_weight_df = following__edge_weight_df.sort_values(['source_screen_name','destination_screen_name'],\n",
    "                                                                 ascending=[True,False])\n",
    "print(following__edge_weight_df.shape)\n",
    "following__edge_weight_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VERTICES DATA\n",
    "node_df = pd.read_csv('nodes_list_retweet_thresh_10.csv').drop(columns=['Unnamed: 0'],axis=1).rename(columns={'Twitter_users':'screen_name'})\n",
    "retweet_node_df = pd.merge(tweet_df[['screen_name','retweet_count']],node_df,on='screen_name',how='inner')\n",
    "retweet_node_df = retweet_node_df.groupby('screen_name').sum()\n",
    "retweet_node_df.reset_index(inplace=True)\n",
    "retweet_node_df['retweet_count'] = retweet_node_df['retweet_count'].astype(float)\n",
    "retweet_node_df.to_csv('map_equation_vertices.csv')\n",
    "retweet_node_df.head()\n",
    "\n",
    "#LINK DATA\n",
    "retweet_edge_df = following__edge_weight_df[['source_screen_name','destination_screen_name','edge_weight']]\n",
    "retweet_edge_df.reset_index(drop=True,inplace=True)\n",
    "retweet_edge_df['source_index'] = ''\n",
    "retweet_edge_df['dest_index'] = ''\n",
    "df = retweet_edge_df.copy()\n",
    "for ind,row in df.iterrows():\n",
    "    source_name = row['source_screen_name']\n",
    "    source_index = retweet_node_df[retweet_node_df['screen_name']==source_name].index.values.astype(int)[0]\n",
    "    df.at[ind,'source_index'] = source_index\n",
    "    \n",
    "    dest_name = row['destination_screen_name']\n",
    "    dest_index = retweet_node_df[retweet_node_df['screen_name']==dest_name].index.values.astype(int)[0]\n",
    "    df.at[ind,'dest_index'] = dest_index\n",
    "    \n",
    "js_link_df = df[['source_index','dest_index','edge_weight']]\n",
    "#js_link_df.to_csv('map_equation_edges.csv')\n",
    "print(js_link_df.shape)\n",
    "js_link_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directed_link_json_list = []\n",
    "for index,row in js_link_df.iterrows():\n",
    "    new_dict = {'source':row['source_index'],\n",
    "                'target':row['dest_index'],\n",
    "                'weight':row['edge_weight']}\n",
    "    directed_link_json_list.append(new_dict)\n",
    "directed_link_json_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes_week_lists = [undirected_node_json_list_week1,undirected_node_json_list_week2,\n",
    "                        undirected_node_json_list_week3,undirected_node_json_list_week4]\n",
    "for i in range(4):\n",
    "    js_json_data = {'nodes':all_nodes_week_lists[i],'links':directed_link_json_list}\n",
    "    with open('directed_json_data_week_'+str(i+1)+'.json', 'w') as f:\n",
    "        print(js_json_data, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(directed_link_json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
